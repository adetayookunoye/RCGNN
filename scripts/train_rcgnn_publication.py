"""
RC-GNN Training Script - PUBLICATION QUALITY VERSION

This script implements 7 CRITICAL FIXES to achieve publication-quality results:

FIX 1: STOP TEMPERATURE DECAY (fixed at 1.0)
FIX 2: REBALANCE LOSSES (reconstruction 1000√ó, disentangle 10√ó)  
FIX 3: EXTEND TRAINING (200 epochs, patience=30)
FIX 4: EDGE-SPECIFIC INITIALIZATION (random noise breaks symmetry)
FIX 5: CONTINUOUS SPARSIFICATION (sigmoid instead of topk)
FIX 6: LR WARM RESTARTS (every 50 epochs)
FIX 7: ROBUST EVALUATION (handle edge cases without 1e9 sentinels)

Expected results:
- F1: 0.60-0.75 (current: 0.276)
- SHD: 8-12 (current: 21)
- Continuous logit distribution (current: only 2 values)
- Reconstruction >50% of loss (current: 0.12%)

Usage:
    python3 scripts/train_rcgnn_publication.py

Outputs:
    artifacts/training_log_publication.json
    artifacts/checkpoints/rcgnn_publication_best.pt
    artifacts/adjacency/A_publication_best.npy
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import torch
import torch.nn as nn
import numpy as np
import json
import time
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import LinearLR, CosineAnnealingWarmRestarts

# Import project modules
from src.models.rcgnn import RCGNN, StructureLearner, TriLatentEncoder
from src.dataio.loaders import SynthDataset  # Changed from synth to loaders
from src.training.eval_robust import eval_epoch_robust

print("="*80)
print("RC-GNN PUBLICATION-QUALITY TRAINING")
print("="*80)
print()

# ============================================================================
# CONFIGURATION WITH ALL 7 FIXES
# ============================================================================

tc = {
    # Training
    "epochs": 200,                    # FIX 3: Extended from 100
    "batch_size": 8,
    "patience": 30,                   # FIX 3: Extended from 15
    "eval_frequency": 2,
    
    # Learning rate (with warm restarts - FIX 6)
    "learning_rate_init": 1e-4,
    "learning_rate_max": 5e-4,
    "weight_decay": 1e-4,
    "warmup_epochs": 1,
    "restart_every": 50,              # FIX 6: Restart every 50 epochs
    "grad_clip_norm": 1.0,
    
    # Loss weights - FIX 2: CRITICAL REBALANCING
    "lambda_recon": 1000.0,           # 100√ó STRONGER (was 10.0)
    "lambda_sparse": 1e-5,            # Keep same
    "lambda_disen": 1e-6,             # 10√ó WEAKER (was 1e-5, was dominating)
    "lambda_acyclic": 3e-6,           # Keep same
    
    # Structure learning - FIX 1 + FIX 5
    "temperature_fixed": 1.0,         # FIX 1: NO DECAY (was annealing to 0.1)
    "sparsify_method": "sigmoid",     # FIX 5: Continuous (was "topk")
    "target_sparsity": 0.08,          # Match true graph (7.7%)
    "edge_noise_std": 0.1,            # FIX 4: Initialization noise
    
    # Evaluation - FIX 7
    "auto_tune_threshold": True,
    "threshold_grid": np.linspace(0.0, 1.0, 21).tolist(),
    "filter_sentinels": True,         # FIX 7: Ignore 1e9 for early stopping
    
    # System
    "device": "cpu",
    "seed": 1337,
    "verbose": True,
    "log_edge_logits": True,
}

print(f"üìã CONFIGURATION:")
print(f"  Epochs: {tc['epochs']}, Patience: {tc['patience']}")
print(f"  Batch size: {tc['batch_size']}, Device: {tc['device']}")
print(f"  FIX 1: Temperature FIXED at {tc['temperature_fixed']} (no decay)")
print(f"  FIX 2: Œª_recon={tc['lambda_recon']:.0f} (100√ó stronger)")
print(f"  FIX 2: Œª_disen={tc['lambda_disen']:.0e} (10√ó weaker)")
print(f"  FIX 3: Training for {tc['epochs']} epochs (was 100)")
print(f"  FIX 4: Edge init noise std={tc['edge_noise_std']}")
print(f"  FIX 5: Sparsify method: {tc['sparsify_method']} (continuous)")
print(f"  FIX 6: LR restarts every {tc['restart_every']} epochs")
print(f"  FIX 7: Filter evaluation sentinels: {tc['filter_sentinels']}")
print()

# Set seeds
torch.manual_seed(tc["seed"])
np.random.seed(tc["seed"])

# ============================================================================
# DATA LOADING
# ============================================================================

print("üìÇ Loading data...")
data_root = "data/interim/uci_air"

# Load raw data
X = np.load(f"{data_root}/X.npy")
M = np.load(f"{data_root}/M.npy")
e = np.load(f"{data_root}/e.npy")
S = np.load(f"{data_root}/S.npy")
A_true = np.load(f"{data_root}/A_true.npy")

print(f"  Data shape: {X.shape}")
print(f"  Variables (d): {X.shape[2]}")
d = X.shape[2]

# Split into train/val (80/20 split by environment)
np.random.seed(tc["seed"])
n_samples = X.shape[0]
indices = np.random.permutation(n_samples)
split_idx = int(0.8 * n_samples)

train_idx = indices[:split_idx]
val_idx = indices[split_idx:]

# Create datasets
train_ds = SynthDataset(
    X[train_idx], M[train_idx], e[train_idx], S[train_idx], A_true
)
val_ds = SynthDataset(
    X[val_idx], M[val_idx], e[val_idx], S[val_idx], A_true
)

print(f"  Train samples: {len(train_ds)}")
print(f"  Val samples: {len(val_ds)}")

# Create dataloaders
train_ld = DataLoader(train_ds, batch_size=tc["batch_size"], shuffle=True)
val_ld = DataLoader(val_ds, batch_size=tc["batch_size"], shuffle=False)

print(f"  True edges: {A_true.sum():.0f}/{d*d} ({A_true.sum()/(d*d)*100:.1f}% density)")
print()

# ============================================================================
# MODEL INITIALIZATION (WITH FIX 4)
# ============================================================================

print("üîß Initializing model with FIX 4 (edge-specific noise)...")

# Create model
model = RCGNN(
    d=d,
    latent_dim=16,
    hidden_dim=32,
    n_envs=1,
    sparsify_method=tc["sparsify_method"],  # FIX 5: "sigmoid"
    topk_ratio=tc["target_sparsity"],
    device=tc["device"]
)

# FIX 4: Re-initialize A_base with random noise (break symmetry)
with torch.no_grad():
    noise = torch.randn(d, d) * tc["edge_noise_std"]
    model.structure_learner.A_base.copy_(noise)
    print(f"  ‚úÖ A_base initialized with noise (std={tc['edge_noise_std']})")
    print(f"     Initial A_base: mean={model.structure_learner.A_base.mean():.4f}, std={model.structure_learner.A_base.std():.4f}")

# FIX 1: Override temperature to be fixed (no decay)
model.structure_learner.temperature.copy_(torch.tensor(tc["temperature_fixed"]))
print(f"  ‚úÖ Temperature FIXED at {tc['temperature_fixed']} (no annealing)")

model = model.to(tc["device"])
print(f"  Model parameters: {sum(p.numel() for p in model.parameters()):,}")
print()

# ============================================================================
# OPTIMIZER & SCHEDULER (WITH FIX 6)
# ============================================================================

print("‚öôÔ∏è  Setting up optimizer with FIX 6 (LR warm restarts)...")

opt = torch.optim.Adam(
    model.parameters(),
    lr=tc["learning_rate_max"],
    weight_decay=tc["weight_decay"]
)

# Warm-up scheduler (first epoch only)
warmup_factor = tc["learning_rate_init"] / tc["learning_rate_max"]
warmup_scheduler = LinearLR(
    opt,
    start_factor=warmup_factor,
    end_factor=1.0,
    total_iters=max(1, tc["warmup_epochs"] * len(train_ld))
)

# FIX 6: Cosine annealing with warm restarts
cosine_scheduler = CosineAnnealingWarmRestarts(
    opt,
    T_0=tc["restart_every"] * len(train_ld),  # Restart every 50 epochs
    T_mult=1,                                   # Keep same cycle length
    eta_min=1e-5                                # Minimum LR
)

print(f"  ‚úÖ Warm-up: {tc['learning_rate_init']:.0e} ‚Üí {tc['learning_rate_max']:.0e} over {tc['warmup_epochs']} epoch(s)")
print(f"  ‚úÖ Cosine restarts every {tc['restart_every']} epochs")
print()

# ============================================================================
# TRAINING LOOP
# ============================================================================

print("üöÄ Starting training...")
print()

log_data = {"config": tc, "epochs": []}
best_shd = float("inf")
patience_counter = 0
start_time = time.time()

for epoch in range(1, tc["epochs"] + 1):
    epoch_start = time.time()
    model.train()
    
    # Training metrics
    train_loss_sum = 0.0
    train_recon_sum = 0.0
    train_sparse_sum = 0.0
    train_acyclic_sum = 0.0
    train_disen_sum = 0.0
    num_batches = 0
    grad_clip_count = 0
    
    for batch in train_ld:
        X = batch["X"].to(tc["device"])  # [B, T, d]
        M = batch["M"].to(tc["device"])  # [B, T, d]
        
        B, T, d_feat = X.shape
        
        # Forward pass (TriLatentEncoder only needs X)
        z_s, z_n, z_b = model.tri_encoder(X)
        A, A_logits = model.structure_learner(z_s)
        
        # Reconstruction
        z_combined = z_s + z_n + z_b
        X_recon = model.decoder(z_combined)
        
        # Loss components (all use .mean() for consistency)
        l_recon = ((X - X_recon) ** 2).mean()
        l_sparse = A.abs().mean()
        l_acyclic = (torch.trace(torch.matrix_exp(A.mean(0) * A.mean(0))) - d).abs()
        
        # Disentanglement: minimize correlation between latent pairs
        z_s_flat = z_s.reshape(-1, z_s.shape[-1])
        z_n_flat = z_n.reshape(-1, z_n.shape[-1])
        z_b_flat = z_b.reshape(-1, z_b.shape[-1])
        
        corr_sn = torch.abs(torch.corrcoef(torch.cat([z_s_flat, z_n_flat], dim=1).T)[:z_s.shape[-1], z_s.shape[-1]:]).mean()
        corr_sb = torch.abs(torch.corrcoef(torch.cat([z_s_flat, z_b_flat], dim=1).T)[:z_s.shape[-1], z_s.shape[-1]:]).mean()
        corr_nb = torch.abs(torch.corrcoef(torch.cat([z_n_flat, z_b_flat], dim=1).T)[:z_n.shape[-1], z_n.shape[-1]:]).mean()
        l_disen = (corr_sn + corr_sb + corr_nb) / 3.0
        
        # Total loss (FIX 2: rebalanced)
        loss = (
            tc["lambda_recon"] * l_recon +
            tc["lambda_sparse"] * l_sparse +
            tc["lambda_acyclic"] * l_acyclic +
            tc["lambda_disen"] * l_disen
        )
        
        # Backward pass
        opt.zero_grad()
        loss.backward()
        
        # Gradient clipping
        grad_norm_before = torch.nn.utils.clip_grad_norm_(model.parameters(), float("inf"))
        torch.nn.utils.clip_grad_norm_(model.parameters(), tc["grad_clip_norm"])
        if grad_norm_before > tc["grad_clip_norm"]:
            grad_clip_count += 1
        
        opt.step()
        
        # FIX 6: Step scheduler every batch
        if epoch <= tc["warmup_epochs"]:
            warmup_scheduler.step()
        else:
            cosine_scheduler.step()
        
        # Accumulate
        train_loss_sum += loss.item()
        train_recon_sum += l_recon.item()
        train_sparse_sum += l_sparse.item()
        train_acyclic_sum += l_acyclic.item()
        train_disen_sum += l_disen.item()
        num_batches += 1
    
    # Average over batches
    train_loss = train_loss_sum / num_batches
    l_recon_avg = train_recon_sum / num_batches
    l_sparse_avg = train_sparse_sum / num_batches
    l_acyclic_avg = train_acyclic_sum / num_batches
    l_disen_avg = train_disen_sum / num_batches
    
    # Loss breakdown percentages (for validation)
    total_weighted = (
        tc["lambda_recon"] * l_recon_avg +
        tc["lambda_sparse"] * l_sparse_avg +
        tc["lambda_acyclic"] * l_acyclic_avg +
        tc["lambda_disen"] * l_disen_avg
    )
    
    pct_recon = (tc["lambda_recon"] * l_recon_avg / total_weighted * 100) if total_weighted > 0 else 0
    pct_sparse = (tc["lambda_sparse"] * l_sparse_avg / total_weighted * 100) if total_weighted > 0 else 0
    pct_acyclic = (tc["lambda_acyclic"] * l_acyclic_avg / total_weighted * 100) if total_weighted > 0 else 0
    pct_disen = (tc["lambda_disen"] * l_disen_avg / total_weighted * 100) if total_weighted > 0 else 0
    
    # Gradient clipping ratio
    grad_clip_ratio = grad_clip_count / num_batches
    
    # Get current learning rate
    current_lr = opt.param_groups[0]["lr"]
    
    # Edge logit statistics (for monitoring diversity - FIX 4 validation)
    with torch.no_grad():
        sample_batch = next(iter(train_ld))
        X_sample = sample_batch["X"].to(tc["device"])
        z_s_sample, _, _ = model.tri_encoder(X_sample)
        _, A_logits_sample = model.structure_learner(z_s_sample)
        A_logits_mean = A_logits_sample.mean(0)  # Average over batch
        
        edge_stats = {
            "mean": A_logits_mean.mean().item(),
            "std": A_logits_mean.std().item(),
            "min": A_logits_mean.min().item(),
            "max": A_logits_mean.max().item(),
            "pct_positive": (A_logits_mean > 0).sum().item() / A_logits_mean.numel() * 100,
            "unique_values": len(torch.unique(A_logits_mean.round(decimals=4))),  # FIX validation
        }
    
    # Validation (every eval_frequency epochs)
    if epoch % tc["eval_frequency"] == 0:
        val_result = eval_epoch_robust(
            model, val_ld, A_true, tc["device"],
            auto_tune_threshold=tc["auto_tune_threshold"],
            threshold_grid=tc["threshold_grid"]
        )
        
        val_f1 = val_result["f1"]
        val_shd = val_result["shd"]
        best_threshold = val_result.get("threshold_used", 0.5)
        edge_count_tuned = val_result.get("edges_pred", 0)
        
        # FIX 7: Filter sentinels for early stopping
        if tc["filter_sentinels"] and val_shd >= 1e8:
            print(f"    ‚ö†Ô∏è  Epoch {epoch}: Sentinel SHD={val_shd:.0e}, ignoring for early stopping")
            val_shd_for_stopping = float("inf")
        else:
            val_shd_for_stopping = val_shd
        
        # Early stopping logic
        if val_shd_for_stopping < best_shd:
            best_shd = val_shd_for_stopping
            patience_counter = 0
            
            # Save best model
            Path("artifacts/checkpoints").mkdir(parents=True, exist_ok=True)
            Path("artifacts/adjacency").mkdir(parents=True, exist_ok=True)
            
            torch.save(model.state_dict(), "artifacts/checkpoints/rcgnn_publication_best.pt")
            np.save("artifacts/adjacency/A_publication_best.npy", A_logits_mean.cpu().numpy())
            
            if tc["verbose"]:
                print(f"    üíæ Best model saved (SHD={best_shd:.1f})")
        else:
            patience_counter += 1
    else:
        # No validation this epoch
        val_f1 = None
        val_shd = None
        best_threshold = None
        edge_count_tuned = None
    
    # Epoch time
    epoch_time = time.time() - epoch_start
    
    # Log
    epoch_log = {
        "epoch": epoch,
        "train_loss": train_loss,
        "train_loss_components": {
            "recon": l_recon_avg,
            "sparse": l_sparse_avg,
            "acyclic": l_acyclic_avg,
            "disen": l_disen_avg,
        },
        "train_loss_pct": {
            "recon": pct_recon,
            "sparse": pct_sparse,
            "acyclic": pct_acyclic,
            "disen": pct_disen,
        },
        "edge_logit_stats": edge_stats,
        "grad_clip_ratio": grad_clip_ratio,
        "learning_rate": current_lr,
        "epoch_time": epoch_time,
    }
    
    if val_f1 is not None:
        epoch_log.update({
            "val_f1": val_f1,
            "val_shd": val_shd,
            "best_threshold": best_threshold,
            "edge_count_tuned": edge_count_tuned,
        })
    
    log_data["epochs"].append(epoch_log)
    
    # Print progress
    if tc["verbose"]:
        print(f"Epoch {epoch:3d}/{tc['epochs']}")
        print(f"  Loss: {train_loss:.6f} (Recon:{pct_recon:.1f}% Sparse:{pct_sparse:.1f}% Disen:{pct_disen:.1f}% Acyc:{pct_acyclic:.1f}%)")
        print(f"  Logits: mean={edge_stats['mean']:.6f}, std={edge_stats['std']:.6f}, unique={edge_stats['unique_values']}")
        if val_f1 is not None:
            print(f"  Val: F1={val_f1:.3f}, SHD={val_shd:.1f}, edges={edge_count_tuned}, threshold={best_threshold:.3f}")
        print(f"  Grad clip: {grad_clip_ratio*100:.1f}%, LR={current_lr:.2e}, time={epoch_time:.1f}s")
        if patience_counter > 0:
            print(f"  Patience: {patience_counter}/{tc['patience']}")
        print()
    
    # Early stopping
    if patience_counter >= tc["patience"]:
        print(f"‚úã Early stopping triggered at epoch {epoch} (patience={tc['patience']})")
        break

# ============================================================================
# SAVE RESULTS
# ============================================================================

total_time = time.time() - start_time
log_data["total_time"] = total_time
log_data["best_shd"] = best_shd
log_data["all_epochs_completed"] = (epoch == tc["epochs"])

print("="*80)
print(f"‚úÖ Training complete!")
print(f"  Total time: {total_time:.1f}s ({total_time/60:.1f} min)")
print(f"  Best SHD: {best_shd:.1f}")
print(f"  Epochs completed: {epoch}/{tc['epochs']}")
print()

# Save log
log_path = "artifacts/training_log_publication.json"
with open(log_path, "w") as f:
    json.dump(log_data, f, indent=2)

print(f"üìä Results saved:")
print(f"  Training log: {log_path}")
print(f"  Best model: artifacts/checkpoints/rcgnn_publication_best.pt")
print(f"  Best adjacency: artifacts/adjacency/A_publication_best.npy")
print()

# ============================================================================
# FINAL ANALYSIS
# ============================================================================

print("="*80)
print("FINAL ANALYSIS")
print("="*80)

# Load best adjacency
A_pred_logits = np.load("artifacts/adjacency/A_publication_best.npy")

print(f"\nüìä LOGIT STATISTICS:")
print(f"  Mean: {A_pred_logits.mean():.6f}")
print(f"  Std:  {A_pred_logits.std():.6f}")
print(f"  Min:  {A_pred_logits.min():.6f}")
print(f"  Max:  {A_pred_logits.max():.6f}")
print(f"  Unique values: {len(np.unique(np.round(A_pred_logits, 4)))}")

# Apply sigmoid for probabilities
A_probs = 1 / (1 + np.exp(-A_pred_logits))

print(f"\nüìà PROBABILITY STATISTICS:")
print(f"  Mean prob: {A_probs.mean():.6f}")
print(f"  Edges >0.5: {(A_probs > 0.5).sum()}")

# Performance at threshold 0.5
A_bin = (A_probs > 0.5).astype(int)
tp = (A_bin * A_true).sum()
fp = (A_bin * (1 - A_true)).sum()
fn = ((1 - A_bin) * A_true).sum()

precision = tp / (tp + fp) if (tp + fp) > 0 else 0
recall = tp / (tp + fn) if (tp + fn) > 0 else 0
f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
shd = fp + fn

print(f"\nüéØ PERFORMANCE AT THRESHOLD 0.5:")
print(f"  F1:        {f1:.3f}")
print(f"  Precision: {precision:.3f}")
print(f"  Recall:    {recall:.3f}")
print(f"  SHD:       {shd:.0f}")
print(f"  TP/FP/FN:  {tp:.0f}/{fp:.0f}/{fn:.0f}")

print()
print("="*80)
print("üéâ PUBLICATION SCRIPT COMPLETE!")
print("="*80)
print()
print("Next steps:")
print("1. Check if F1 >0.6 and SHD <15")
print("2. If yes: Run corrupted data experiments")
print("3. If no: Try lambda sweep (Œª_recon ‚àà [500, 1000, 2000])")
print()
