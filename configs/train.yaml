epochs: 50
batch_size: 8
learning_rate: 0.001
weight_decay: 1e-5
device: "cpu"
seed: 1337
verbose: true

# Loss weights
lambda_recon: 1.0
lambda_sparse: 0.005
lambda_acyclic: 0.05
lambda_disen: 0.01
lambda_inv_mask: 1e-3  # V8.18: MNAR mask-invariance penalty
target_sparsity: 0.1

# Optional supervised adjacency loss (if A_true.npy is available)
# Set >0.0 to nudge A toward ground truth during training (e.g., 0.1 - 0.5)
lambda_supervised: 0.0
